{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "    Restore the extracted event text from the raw sentence, e.g. event text: 'the chocolate be the best', restore text: 'the chocolate are the best', raw sentence text: 'however the chocolate are the best'.\n",
    "\n",
    "    Our goal is finding ASER event's corresponding sub-sentence. Restore strategy is matching head and tail words, and also checking if there exist a proportionate number of overlapped words.\n",
    "\n",
    "    Through continuous iterations with error analysis and adding rules, the final matching success rate is over 95%.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "677461a7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d55c1ab3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import tqdm\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "from mlconjug3 import Conjugator\n",
    "import re\n",
    "\n",
    "# get verb tense\n",
    "\n",
    "def get_verb_forms(conjugator, verb_word: str):\n",
    "    \"\"\"\n",
    "    compute all forms of a verb.\n",
    "\n",
    "    :param\n",
    "    :return: tuple, (present, third-person present, past, present continuous, present perfect)\n",
    "    \"\"\"\n",
    "    present, third_person_present, past, present_continuous, present_perfect = '', '', '', '', ''\n",
    "\n",
    "    try:\n",
    "        for form in conjugator.conjugate(verb_word):\n",
    "            if len(form)==4:\n",
    "                _, tense, sub, verb = form\n",
    "                if sub in ('we','they','you'):\n",
    "                    continue\n",
    "                if tense == 'indicative present' and sub == 'I':\n",
    "                    present = verb\n",
    "                if tense == 'indicative present' and sub == 'he/she/it':\n",
    "                    third_person_present = verb\n",
    "                if tense == 'indicative past tense' and sub == 'I':\n",
    "                    past = verb\n",
    "                if tense == 'indicative present continuous' and sub == 'I':\n",
    "                    present_continuous = verb\n",
    "                if tense == 'indicative present perfect' and sub == 'I':\n",
    "                    present_perfect = verb\n",
    "            else: # 3\n",
    "                continue\n",
    "        stat = 0\n",
    "    except:\n",
    "        #print(traceback.format_exc(()))\n",
    "        stat = -1\n",
    "\n",
    "    return (present, third_person_present, past, present_continuous, present_perfect), stat\n",
    "\n",
    "source_dir='./'\n",
    "\n",
    "conjugator = Conjugator(language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9bec356",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "VERSION = 'v2'\n",
    "\n",
    "aser = joblib.load(source_dir+'Data/amazon_food_review_aser_event_0_100000.%s'%VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f74d7932",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentences': ['I sent these as part of my donation to our service members and I have gotten back some very positive thanks for this thoughtful gift',\n",
       "  \"It's no secret that Coffee is a favorite drink in military deployed environments and just about everybody sends different types of coffee to our men and women in uniform\",\n",
       "  'I wanted to think outside the box and send something a bit different so I sent the best coffee accompanying treat and this was the big hit',\n",
       "  'I am not reviewing this item from how much I like it, I am reviewing this item on how many service members this treat helped',\n",
       "  'A little goes a long way and I say this went a long way',\n",
       "  \"Plus you can't beat the price with free shipping to our service members\"],\n",
       " 'aser': [[i have get back some positive thanks for gift],\n",
       "  [it be no secret,\n",
       "   coffee be a favorite drink in environment just,\n",
       "   coffee send different type of coffee man woman],\n",
       "  [i send],\n",
       "  [i be not review this item,\n",
       "   i like it,\n",
       "   i be review this item,\n",
       "   service member help],\n",
       "  [a little go, i say, this go a long way],\n",
       "  [you can not beat the price with shipping to member]]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aser[129784]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "273cd93c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# event_sent_map={} # {event:sentence} \n",
    "# for text_id, info in aser.items():\n",
    "#     for s,event_list in zip(info['sentences'],info['aser']):\n",
    "#         for e in event_list:\n",
    "#             event_sent_map[e]=s\n",
    "\n",
    "# event might appear twice in aser result. Use list not dict\n",
    "\n",
    "event_sent_list=[] # (event,sentence)\n",
    "for text_id, info in aser.items():\n",
    "    for s,event_list in zip(info['sentences'],info['aser']):\n",
    "        for e in event_list:\n",
    "            event_sent_list.append((text_id,e,s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b87089b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "727023"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(event_sent_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c717b5c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# UnitTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8b50788",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # unit test\n",
    "# event_text=aser[534543]['aser'][0][1].__repr__()\n",
    "# sent=aser[534543]['sentences'][0]\n",
    "\n",
    "# hw=event_text.split(' ')[0]\n",
    "# tw=event_text.split(' ')[-1]\n",
    "\n",
    "# start_ind=0\n",
    "# end_ind=-1\n",
    "# for i,w in enumerate(sent.split(' ')):\n",
    "#     if w==hw:\n",
    "#         start_ind=i\n",
    "#     if w==tw:\n",
    "#         end_ind=i\n",
    "\n",
    "# event_raw=' '.join(sent.split(' ')[start_ind:end_ind+1])\n",
    "\n",
    "# print(event_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33a62cf1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "event_text='now i be glad'\n",
    "sent=\"I thought I was buying less than I was, but now I'm glad I got more than I thought because now I'm going to make all my own flatbreads\"\n",
    "\n",
    "for i in range(1):\n",
    "    hw=event_text.split(' ')[0]\n",
    "    hw_pos=event.pos_tags[0]\n",
    "    tw=event_text.split(' ')[-1]\n",
    "    tw_pos=event.pos_tags[-1]\n",
    "\n",
    "    start_ind=0\n",
    "    end_ind=-1\n",
    "    for i,sw in enumerate(sent.split(' ')):\n",
    "        # (1) head word and tail word matching\n",
    "        if sw==hw:\n",
    "            start_ind=i\n",
    "        if sw==tw:\n",
    "            end_ind=i\n",
    "        # (2) upper/lower case matching       \n",
    "        if sw.lower()==hw:\n",
    "            start_ind=i\n",
    "        if sw.lower()==tw:\n",
    "            end_ind=i\n",
    "        # (3) plural matching\n",
    "        if 'NN' in hw_pos:\n",
    "            blob = TextBlob(hw)\n",
    "            tmp=[word.pluralize() for word in blob.words]\n",
    "            hw_plural = tmp[0] if len(tmp)>0 else ''\n",
    "            if sw==hw_plural:\n",
    "                start_ind=i\n",
    "        if 'NN' in tw_pos:\n",
    "            blob = TextBlob(tw)\n",
    "            tmp=[word.pluralize() for word in blob.words]\n",
    "            tw_plural = tmp[0] if len(tmp)>0 else ''\n",
    "            if sw==tw_plural:\n",
    "                end_ind=i\n",
    "                \n",
    "        # (4) verb-tense matching\n",
    "        if 'VB' in hw_pos and hw in verb_tense:\n",
    "            tmp=verb_tense[hw]\n",
    "            for t in tmp:\n",
    "                if sw==t:\n",
    "                    start_ind=i\n",
    "                    break\n",
    "        if 'VB' in tw_pos and tw in verb_tense:\n",
    "            tmp=verb_tense[tw]\n",
    "            for t in tmp:\n",
    "                if sw==t:\n",
    "                    end_ind=i\n",
    "                    break\n",
    "\n",
    "    event_raw=' '.join(sent.split(' ')[start_ind:end_ind+1])\n",
    "\n",
    "\n",
    "print(event_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ed2ea6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37a40cf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d530d12",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 727023/727023 [00:01<00:00, 415393.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get all verbs from events\n",
    "verbs=set()\n",
    "\n",
    "for text_id,event, sent in tqdm.tqdm(event_sent_list):\n",
    "\n",
    "    for w, pt in zip(event.__repr__().split(' '),event.pos_tags):\n",
    "\n",
    "        if 'VB' in pt:\n",
    "\n",
    "            verbs.add(w)\n",
    "\n",
    "verbs=list(verbs)\n",
    "print(len(verbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5699b83f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5155/5155 [00:24<00:00, 208.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "verb_tense={}\n",
    "fail=[]\n",
    "for v in tqdm.tqdm(verbs):\n",
    "    tmp,status=get_verb_forms(conjugator,v)\n",
    "    if status==-1:\n",
    "        fail.append(v)\n",
    "    else:\n",
    "        verb_tense[v]=tmp\n",
    "print(len(fail))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a09c93",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "    From fail cases, we find most of them have typo-errors. No need to bother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e4fdd344",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "personal_pronoun={\n",
    "    'i': ['i','I','me'],\n",
    "    'you': ['you','your'],\n",
    "    'he': ['he','him','his'],\n",
    "    'she': ['she','her'],\n",
    "    'it': ['it','its'],\n",
    "    'we': ['we','us','our'],\n",
    "    'they': ['they','them','their'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "30441bba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean(sentence:str):\n",
    "    res=sentence.replace(\"'ve\",' have').replace(\"'ll\",' will').replace(\"'re\",' are').replace(\"'m\",' am').replace(\"'d\",' would').replace(\"n't\",' not')\n",
    "\n",
    "    res=res.replace(\"he's\",'he has').replace(\"she's\",'she has').replace(\"it's\",'it is')\n",
    "\n",
    "    res=res.replace(\"He's\",'He has').replace(\"She's\",'She has').replace(\"It's\",'It is')\n",
    "\n",
    "    res=res.replace(\"<br />\",' ').replace(\"<br/>\",' ')\n",
    "\n",
    "    res=res.replace(\"(\",' ').replace(\")\",' ') # this apple (brand is blablabla)\n",
    "\n",
    "    res=res.replace(\";\",'.').replace(\":\",'.') # E.g.: I was excited to find these two teas because they are the perfect compromise: just enough caffeine to keep me going, but not enough to make my heart pound\n",
    "    \n",
    "    res=res.replace('\"',' ') # E.g.: I do not care for the \"Creamy Peanut Butter\"\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5ee5d5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Round1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1733ff39",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 857.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total events: 727023, processed events: 24, ratio: 0.00%\n",
      "Unprocessed Samples: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 72.3%\n",
    "\n",
    "event_processed=[] # (event, raw_event_text, sent)\n",
    "event_unprocessed=[] # (event,'', sent)\n",
    "\n",
    "for text_id, event, sent in tqdm.tqdm(event_sent_list[:30]):\n",
    "    event_text=event.__repr__()\n",
    "    hw=event_text.split(' ')[0]\n",
    "    hw_pos=event.pos_tags[0]\n",
    "    tw=event_text.split(' ')[-1]\n",
    "    tw_pos=event.pos_tags[-1]\n",
    "\n",
    "    start_ind=0\n",
    "    end_ind=-1\n",
    "    sub_sents=sent.split(',')\n",
    "    for ss in sub_sents:\n",
    "        ss_cut=ss.split(' ')\n",
    "        for i,sw in enumerate(ss_cut):\n",
    "            # (1) head word and tail word matching\n",
    "            if sw==hw:\n",
    "                start_ind=i\n",
    "            if sw==tw:\n",
    "                end_ind=i\n",
    "            # (2) upper/lower case matching       \n",
    "            if sw.lower()==hw:\n",
    "                start_ind=i\n",
    "            if sw.lower()==tw:\n",
    "                end_ind=i\n",
    "            # (3) plural matching\n",
    "            if 'NN' in hw_pos:\n",
    "                blob = TextBlob(hw)\n",
    "                tmp=[word.pluralize() for word in blob.words]\n",
    "                hw_plural = tmp[0] if len(tmp)>0 else ''\n",
    "                if sw==hw_plural:\n",
    "                    start_ind=i\n",
    "            if 'NN' in tw_pos:\n",
    "                blob = TextBlob(tw)\n",
    "                tmp=[word.pluralize() for word in blob.words]\n",
    "                tw_plural = tmp[0] if len(tmp)>0 else ''\n",
    "                if sw==tw_plural:\n",
    "                    end_ind=i\n",
    "            # (4) verb-tense matching\n",
    "            if 'VB' in hw_pos and hw in verb_tense:\n",
    "                tmp=verb_tense[hw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        start_ind=i\n",
    "                        break\n",
    "            if 'VB' in tw_pos and tw in verb_tense:\n",
    "                tmp=verb_tense[tw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        end_ind=i\n",
    "                        break\n",
    "            \n",
    "\n",
    "\n",
    "        event_raw_text=' '.join(ss_cut[start_ind:end_ind+1])\n",
    "\n",
    "        if len(event_raw_text)>0:\n",
    "            event_processed.append((text_id, event,event_raw_text,sent))\n",
    "            break # no need to search next sub-sentence\n",
    "    if len(event_raw_text)==0:\n",
    "        event_unprocessed.append((text_id, event,'',sent))\n",
    "\n",
    "\n",
    "# tracking\n",
    "print(\"Total events: %d, processed events: %d, ratio: %.2f%%\"%(len(event_sent_map), len(event_processed), 100*len(event_processed)/len(event_sent_map)))\n",
    "\n",
    "print(\"Unprocessed Samples: \")\n",
    "\n",
    "# __tmp=random.sample([(t,e,s) for t,e,_,s in event_unprocessed],10)\n",
    "# for t,e,s in __tmp:\n",
    "#     print(\"textID %s: %s == %s\"%(t,e.__repr__(),s))\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cb2251",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Round2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123d3ace",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# upgrade the final matching mechanism\n",
    "\n",
    "# update word-matching rule: personal pronoun;\n",
    "\n",
    "event_processed=[] # (event, raw_event_text, sent)\n",
    "event_unprocessed=[] # (event,'', sent)\n",
    "\n",
    "for text_id, event, sent in tqdm.tqdm(event_sent_list[:1000]):\n",
    "    event_text=event.__repr__()\n",
    "    hw=event_text.split(' ')[0]\n",
    "    hw_pos=event.pos_tags[0]\n",
    "    tw=event_text.split(' ')[-1]\n",
    "    tw_pos=event.pos_tags[-1]\n",
    "\n",
    "    sub_sents=sent.split(',')\n",
    "\n",
    "    match_res=[]\n",
    "\n",
    "    for ss in sub_sents:\n",
    "        ss_cut=ss.split(' ')\n",
    "\n",
    "        start_inds=set()\n",
    "        end_inds=set()\n",
    "\n",
    "        for i,sw in enumerate(ss_cut):\n",
    "            # (1) head word and tail word matching\n",
    "            if sw==hw:\n",
    "                start_ind=i\n",
    "                start_inds.add(start_ind)\n",
    "            if sw==tw:\n",
    "                end_ind=i\n",
    "                end_inds.add(end_ind)\n",
    "            # (2) upper/lower case matching       \n",
    "            if sw.lower()==hw:\n",
    "                start_ind=i\n",
    "                start_inds.add(start_ind)\n",
    "\n",
    "            if sw.lower()==tw:\n",
    "                end_ind=i\n",
    "                end_inds.add(end_ind)\n",
    "\n",
    "            # (3) plural matching\n",
    "            if 'NN' in hw_pos:\n",
    "                blob = TextBlob(hw)\n",
    "                tmp=[word.pluralize() for word in blob.words]\n",
    "                hw_plural = tmp[0] if len(tmp)>0 else ''\n",
    "                if sw==hw_plural:\n",
    "                    start_ind=i\n",
    "                    start_inds.add(start_ind)\n",
    "\n",
    "            if 'NN' in tw_pos:\n",
    "                blob = TextBlob(tw)\n",
    "                tmp=[word.pluralize() for word in blob.words]\n",
    "                tw_plural = tmp[0] if len(tmp)>0 else ''\n",
    "                if sw==tw_plural:\n",
    "                    end_ind=i\n",
    "                    end_inds.add(end_ind)\n",
    "\n",
    "            # (4) verb-tense matching\n",
    "            if 'VB' in hw_pos and hw in verb_tense:\n",
    "                tmp=verb_tense[hw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        start_ind=i\n",
    "                        start_inds.add(start_ind)\n",
    "\n",
    "            if 'VB' in tw_pos and tw in verb_tense:\n",
    "                tmp=verb_tense[tw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        end_ind=i\n",
    "                        end_inds.add(end_ind)\n",
    "\n",
    "            # (5) personal pronoun matching\n",
    "            if hw in personal_pronoun:\n",
    "                tmp=personal_pronoun[hw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        start_ind=i\n",
    "                        start_inds.add(start_ind)\n",
    "\n",
    "            if tw in personal_pronoun:\n",
    "                tmp=personal_pronoun[tw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        end_ind=i\n",
    "                        end_inds.add(end_ind)\n",
    "\n",
    "        \n",
    "        match_res.append((ss_cut, start_inds, end_inds))\n",
    "\n",
    "    # determine the final start and end indices\n",
    "    # loop over all possible combinations of start and end indices and find the one with the maximum overlap inside\n",
    "    event_inner_cut=event_text.split(' ')[1:-1]\n",
    "    start_ind=-1\n",
    "    end_ind=-1\n",
    "    best_ss_cnt=[]\n",
    "    inner_match_cnt=0\n",
    "    for ss_cut, start_inds, end_inds in match_res:\n",
    "        for s in start_inds:\n",
    "            for e in end_inds:\n",
    "                if s<e:\n",
    "                    cnt=0\n",
    "                    for _x in event_inner_cut:\n",
    "                        if _x in ss_cut[s+1:e]:\n",
    "                            cnt+=1\n",
    "                    if cnt>=inner_match_cnt:\n",
    "                        inner_match_cnt=cnt\n",
    "                        start_ind=s\n",
    "                        end_ind=e\n",
    "                        best_ss_cnt=ss_cut\n",
    "\n",
    "    event_raw_text=' '.join(best_ss_cnt[start_ind:end_ind+1])\n",
    "\n",
    "    if len(event_raw_text)>0:\n",
    "        event_processed.append((text_id, event, event_raw_text, sent))\n",
    "    if len(event_raw_text)==0:\n",
    "        event_unprocessed.append((text_id, event,'',sent))\n",
    "\n",
    "\n",
    "# tracking\n",
    "print(\"Total events: %d, processed events: %d, ratio: %.2f%%\"%(len(event_sent_map), len(event_processed), 100*len(event_processed)/len(event_sent_map)))\n",
    "\n",
    "print(\"Unprocessed Samples: \")\n",
    "\n",
    "__tmp=random.sample([(t,e,s) for t,e,_,s in event_unprocessed],10)\n",
    "for t,e,s in __tmp:\n",
    "    print(\"textID %s: %s == %s\"%(t,e.__repr__(),s))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d765674b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Round3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc6de09",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# update: clean sentence\n",
    "\n",
    "event_processed=[] # (event, raw_event_text, sent)\n",
    "event_unprocessed=[] # (event,'', sent)\n",
    "\n",
    "for text_id, event, sent in tqdm.tqdm(event_sent_list[:1000]):\n",
    "    event_text=event.__repr__()\n",
    "    hw=event_text.split(' ')[0]\n",
    "    hw_pos=event.pos_tags[0]\n",
    "    tw=event_text.split(' ')[-1]\n",
    "    tw_pos=event.pos_tags[-1]\n",
    "\n",
    "    sub_sents=clean(sent).split(',')\n",
    "\n",
    "    match_res=[]\n",
    "\n",
    "    for ss in sub_sents:\n",
    "        ss_cut=ss.split(' ')\n",
    "\n",
    "        start_inds=set()\n",
    "        end_inds=set()\n",
    "\n",
    "        for i,sw in enumerate(ss_cut):\n",
    "            sw=sw.lower()\n",
    "\n",
    "            # (1) head word and tail word matching\n",
    "            if sw==hw:\n",
    "                start_ind=i\n",
    "                start_inds.add(start_ind)\n",
    "            if sw==tw:\n",
    "                end_ind=i\n",
    "                end_inds.add(end_ind)\n",
    "            # (2) upper/lower case matching       \n",
    "            # if sw.lower()==hw:\n",
    "            #     start_ind=i\n",
    "            #     start_inds.add(start_ind)\n",
    "\n",
    "            # if sw.lower()==tw:\n",
    "            #     end_ind=i\n",
    "            #     end_inds.add(end_ind)\n",
    "\n",
    "            # (3) plural matching\n",
    "            if 'NN' in hw_pos:\n",
    "                blob = TextBlob(hw)\n",
    "                tmp=[word.pluralize() for word in blob.words]\n",
    "                hw_plural = tmp[0] if len(tmp)>0 else ''\n",
    "                if sw==hw_plural:\n",
    "                    start_ind=i\n",
    "                    start_inds.add(start_ind)\n",
    "\n",
    "            if 'NN' in tw_pos:\n",
    "                blob = TextBlob(tw)\n",
    "                tmp=[word.pluralize() for word in blob.words]\n",
    "                tw_plural = tmp[0] if len(tmp)>0 else ''\n",
    "                if sw==tw_plural:\n",
    "                    end_ind=i\n",
    "                    end_inds.add(end_ind)\n",
    "\n",
    "            # (4) verb-tense matching\n",
    "            if 'VB' in hw_pos and hw in verb_tense:\n",
    "                tmp=verb_tense[hw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        start_ind=i\n",
    "                        start_inds.add(start_ind)\n",
    "\n",
    "            if 'VB' in tw_pos and tw in verb_tense:\n",
    "                tmp=verb_tense[tw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        end_ind=i\n",
    "                        end_inds.add(end_ind)\n",
    "\n",
    "            # (5) personal pronoun matching\n",
    "            if hw in personal_pronoun:\n",
    "                tmp=personal_pronoun[hw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        start_ind=i\n",
    "                        start_inds.add(start_ind)\n",
    "\n",
    "            if tw in personal_pronoun:\n",
    "                tmp=personal_pronoun[tw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        end_ind=i\n",
    "                        end_inds.add(end_ind)\n",
    "\n",
    "        \n",
    "        match_res.append((ss_cut, start_inds, end_inds))\n",
    "\n",
    "    # determine the final start and end indices\n",
    "    # loop over all possible combinations of start and end indices and find the one with the maximum overlap inside\n",
    "    event_inner_cut=event_text.split(' ')[1:-1]\n",
    "    start_ind=-1\n",
    "    end_ind=-1\n",
    "    best_ss_cnt=[]\n",
    "    inner_match_cnt=0\n",
    "    for ss_cut, start_inds, end_inds in match_res:\n",
    "        for s in start_inds:\n",
    "            for e in end_inds:\n",
    "                if s<e:\n",
    "                    cnt=0\n",
    "                    for _x in event_inner_cut:\n",
    "                        if _x in ss_cut[s+1:e]:\n",
    "                            cnt+=1\n",
    "                    if cnt>=inner_match_cnt:\n",
    "                        inner_match_cnt=cnt\n",
    "                        start_ind=s\n",
    "                        end_ind=e\n",
    "                        best_ss_cnt=ss_cut\n",
    "\n",
    "    event_raw_text=' '.join(best_ss_cnt[start_ind:end_ind+1])\n",
    "\n",
    "    if len(event_raw_text)>0:\n",
    "        event_processed.append((text_id, event, event_raw_text, sent))\n",
    "    if len(event_raw_text)==0:\n",
    "        event_unprocessed.append((text_id, event,'',sent))\n",
    "\n",
    "\n",
    "# tracking\n",
    "print(\"Total events: %d, processed events: %d, ratio: %.2f%%\"%(len(event_sent_map), len(event_processed), 100*len(event_processed)/len(event_sent_map)))\n",
    "\n",
    "print(\"Unprocessed Samples: \")\n",
    "\n",
    "__tmp=random.sample([(t,e,s) for t,e,_,s in event_unprocessed],10)\n",
    "for t,e,s in __tmp:\n",
    "    print(\"textID %s: %s == %s\"%(t,e.__repr__(),s))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2f12b7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9867273a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# update: process ;:\n",
    "\n",
    "event_processed=[] # (event, raw_event_text, sent)\n",
    "event_unprocessed=[] # (event,'', sent)\n",
    "\n",
    "for text_id, event, sent in tqdm.tqdm(event_sent_list[:1000]):\n",
    "    event_text=event.__repr__()\n",
    "    hw=event_text.split(' ')[0]\n",
    "    hw_pos=event.pos_tags[0]\n",
    "    tw=event_text.split(' ')[-1]\n",
    "    tw_pos=event.pos_tags[-1]\n",
    "\n",
    "    sub_sents=re.split('\\.|,', clean(sent))\n",
    "\n",
    "    match_res=[]\n",
    "\n",
    "    for ss in sub_sents:\n",
    "        ss_cut=ss.split(' ')\n",
    "\n",
    "        start_inds=set()\n",
    "        end_inds=set()\n",
    "\n",
    "        for i,sw in enumerate(ss_cut):\n",
    "            sw=sw.lower()\n",
    "\n",
    "            # (1) head word and tail word matching\n",
    "            if sw==hw:\n",
    "                start_ind=i\n",
    "                start_inds.add(start_ind)\n",
    "            if sw==tw:\n",
    "                end_ind=i\n",
    "                end_inds.add(end_ind)\n",
    "            # (2) upper/lower case matching       \n",
    "            # if sw.lower()==hw:\n",
    "            #     start_ind=i\n",
    "            #     start_inds.add(start_ind)\n",
    "\n",
    "            # if sw.lower()==tw:\n",
    "            #     end_ind=i\n",
    "            #     end_inds.add(end_ind)\n",
    "\n",
    "            # (3) plural matching\n",
    "            if 'NN' in hw_pos:\n",
    "                blob = TextBlob(hw)\n",
    "                tmp=[word.pluralize() for word in blob.words]\n",
    "                hw_plural = tmp[0] if len(tmp)>0 else ''\n",
    "                if sw==hw_plural:\n",
    "                    start_ind=i\n",
    "                    start_inds.add(start_ind)\n",
    "\n",
    "            if 'NN' in tw_pos:\n",
    "                blob = TextBlob(tw)\n",
    "                tmp=[word.pluralize() for word in blob.words]\n",
    "                tw_plural = tmp[0] if len(tmp)>0 else ''\n",
    "                if sw==tw_plural:\n",
    "                    end_ind=i\n",
    "                    end_inds.add(end_ind)\n",
    "\n",
    "            # (4) verb-tense matching\n",
    "            if 'VB' in hw_pos and hw in verb_tense:\n",
    "                tmp=verb_tense[hw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        start_ind=i\n",
    "                        start_inds.add(start_ind)\n",
    "\n",
    "            if 'VB' in tw_pos and tw in verb_tense:\n",
    "                tmp=verb_tense[tw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        end_ind=i\n",
    "                        end_inds.add(end_ind)\n",
    "\n",
    "            # (5) personal pronoun matching\n",
    "            if hw in personal_pronoun:\n",
    "                tmp=personal_pronoun[hw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        start_ind=i\n",
    "                        start_inds.add(start_ind)\n",
    "\n",
    "            if tw in personal_pronoun:\n",
    "                tmp=personal_pronoun[tw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        end_ind=i\n",
    "                        end_inds.add(end_ind)\n",
    "\n",
    "        \n",
    "        match_res.append((ss_cut, start_inds, end_inds))\n",
    "\n",
    "    # determine the final start and end indices\n",
    "    # loop over all possible combinations of start and end indices and find the one with the maximum overlap inside\n",
    "    event_inner_cut=event_text.split(' ')[1:-1]\n",
    "    start_ind=-1\n",
    "    end_ind=-1\n",
    "    best_ss_cnt=[]\n",
    "    inner_match_cnt=0\n",
    "    for ss_cut, start_inds, end_inds in match_res:\n",
    "        for s in start_inds:\n",
    "            for e in end_inds:\n",
    "                if s<e:\n",
    "                    cnt=0\n",
    "                    for _x in event_inner_cut:\n",
    "                        if _x in ss_cut[s+1:e]:\n",
    "                            cnt+=1\n",
    "                    if cnt>=inner_match_cnt:\n",
    "                        inner_match_cnt=cnt\n",
    "                        start_ind=s\n",
    "                        end_ind=e\n",
    "                        best_ss_cnt=ss_cut\n",
    "\n",
    "    event_raw_text=' '.join(best_ss_cnt[start_ind:end_ind+1])\n",
    "\n",
    "    if len(event_raw_text)>0:\n",
    "        event_processed.append((text_id, event, event_raw_text, sent))\n",
    "    if len(event_raw_text)==0:\n",
    "        event_unprocessed.append((text_id, event,'',sent))\n",
    "\n",
    "\n",
    "# tracking\n",
    "print(\"Total events: %d, processed events: %d, ratio: %.2f%%\"%(len(event_sent_map), len(event_processed), 100*len(event_processed)/len(event_sent_map)))\n",
    "\n",
    "print(\"Unprocessed Samples: \")\n",
    "\n",
    "__tmp=random.sample([(t,e,s) for t,e,_,s in event_unprocessed],10)\n",
    "for t,e,s in __tmp:\n",
    "    print(\"textID %s: %s == %s\"%(t,e.__repr__(),s))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794fce2b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "    96.6% covering rate, good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f380c98",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366acb18",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import tqdm\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "from mlconjug3 import Conjugator\n",
    "import re\n",
    "\n",
    "# get verb tense\n",
    "\n",
    "def get_verb_forms(conjugator, verb_word: str):\n",
    "    \"\"\"\n",
    "    compute all forms of a verb.\n",
    "\n",
    "    :param\n",
    "    :return: tuple, (present, third-person present, past, present continuous, present perfect)\n",
    "    \"\"\"\n",
    "    present, third_person_present, past, present_continuous, present_perfect = '', '', '', '', ''\n",
    "\n",
    "    try:\n",
    "        for form in conjugator.conjugate(verb_word):\n",
    "            if len(form)==4:\n",
    "                _, tense, sub, verb = form\n",
    "                if sub in ('we','they','you'):\n",
    "                    continue\n",
    "                if tense == 'indicative present' and sub == 'I':\n",
    "                    present = verb\n",
    "                if tense == 'indicative present' and sub == 'he/she/it':\n",
    "                    third_person_present = verb\n",
    "                if tense == 'indicative past tense' and sub == 'I':\n",
    "                    past = verb\n",
    "                if tense == 'indicative present continuous' and sub == 'I':\n",
    "                    present_continuous = verb\n",
    "                if tense == 'indicative present perfect' and sub == 'I':\n",
    "                    present_perfect = verb\n",
    "            else: # 3\n",
    "                continue\n",
    "        stat = 0\n",
    "    except:\n",
    "        #print(traceback.format_exc(()))\n",
    "        stat = -1\n",
    "\n",
    "    return (present, third_person_present, past, present_continuous, present_perfect), stat\n",
    "\n",
    "source_dir='./'\n",
    "\n",
    "conjugator = Conjugator(language='en')\n",
    "\n",
    "personal_pronoun={\n",
    "    'i': ['i','I','me'],\n",
    "    'you': ['you','your'],\n",
    "    'he': ['he','him','his'],\n",
    "    'she': ['she','her'],\n",
    "    'it': ['it','its'],\n",
    "    'we': ['we','us','our'],\n",
    "    'they': ['they','them','their'],\n",
    "}\n",
    "\n",
    "def clean(sentence:str):\n",
    "    res=sentence.replace(\"'ve\",' have').replace(\"'ll\",' will').replace(\"'re\",' are').replace(\"'m\",' am').replace(\"'d\",' would').replace(\"n't\",' not')\n",
    "\n",
    "    res=res.replace(\"he's\",'he has').replace(\"she's\",'she has').replace(\"it's\",'it is')\n",
    "\n",
    "    res=res.replace(\"He's\",'He has').replace(\"She's\",'She has').replace(\"It's\",'It is')\n",
    "\n",
    "    res=res.replace(\"<br />\",' ').replace(\"<br/>\",' ')\n",
    "\n",
    "    res=res.replace(\"(\",' ').replace(\")\",' ') # this apple (brand is blablabla)\n",
    "\n",
    "    res=res.replace(\";\",'.').replace(\":\",'.') # E.g.: I was excited to find these two teas because they are the perfect compromise: just enough caffeine to keep me going, but not enough to make my heart pound\n",
    "    \n",
    "    res=res.replace('\"',' ') # E.g.: I do not care for the \"Creamy Peanut Butter\"\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f5baf509",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0_100000 ......\n",
      "Processing 100000_200000 ......\n",
      "Processing 200000_300000 ......\n",
      "Processing 300000_400000 ......\n",
      "Processing 400000_500000 ......\n",
      "Processing 500000_600000 ......\n"
     ]
    }
   ],
   "source": [
    "VERSION = 'v2'\n",
    "\n",
    "aser={}\n",
    "\n",
    "tags=['0_100000','100000_200000','200000_300000','300000_400000','400000_500000','500000_600000']\n",
    "for tag in tags:\n",
    "    print('Processing %s ......'%tag)\n",
    "    aser={**aser,**joblib.load(source_dir+'Data/amazon_food_review_aser_event_%s.%s'%(tag,VERSION))\n",
    "                               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aaade6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "event_sent_list=[] # (text_id, event,sentence)\n",
    "for text_id, info in aser.items():\n",
    "    for s,event_list in zip(info['sentences'],info['aser']):\n",
    "        for e in event_list:\n",
    "            event_sent_list.append((text_id,e,s))\n",
    "\n",
    "# get all verbs from events\n",
    "verbs=set()\n",
    "\n",
    "for text_id,event, sent in tqdm.tqdm(event_sent_list):\n",
    "\n",
    "    for w, pt in zip(event.__repr__().split(' '),event.pos_tags):\n",
    "\n",
    "        if 'VB' in pt:\n",
    "\n",
    "            verbs.add(w)\n",
    "\n",
    "verbs=list(verbs)\n",
    "print(\"Total verbs: %d\"%len(verbs))\n",
    "\n",
    "verb_tense={}\n",
    "fail=[]\n",
    "for v in tqdm.tqdm(verbs):\n",
    "    tmp,status=get_verb_forms(conjugator,v)\n",
    "    if status==-1:\n",
    "        fail.append(v)\n",
    "    else:\n",
    "        verb_tense[v]=tmp\n",
    "print(\"Failing verbs: %d\"%len(fail))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257898e3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# update: process ;:\n",
    "\n",
    "event_processed=[] # (text_id, event, raw_event_text, sub_sent, sent)\n",
    "event_unprocessed=[] # (text_id, event,'', '', sent)\n",
    "\n",
    "for text_id, event, sent in tqdm.tqdm(event_sent_list):\n",
    "    event_text=event.__repr__()\n",
    "    hw=event_text.split(' ')[0]\n",
    "    hw_pos=event.pos_tags[0]\n",
    "    tw=event_text.split(' ')[-1]\n",
    "    tw_pos=event.pos_tags[-1]\n",
    "\n",
    "    sub_sents=re.split('\\.|,', clean(sent))\n",
    "\n",
    "    match_res=[]\n",
    "\n",
    "    for ss in sub_sents:\n",
    "        ss_cut=ss.split(' ')\n",
    "\n",
    "        start_inds=set()\n",
    "        end_inds=set()\n",
    "\n",
    "        for i,sw in enumerate(ss_cut):\n",
    "            sw=sw.lower()\n",
    "\n",
    "            # (1) head word and tail word matching\n",
    "            if sw==hw:\n",
    "                start_ind=i\n",
    "                start_inds.add(start_ind)\n",
    "            if sw==tw:\n",
    "                end_ind=i\n",
    "                end_inds.add(end_ind)\n",
    "            # (2) upper/lower case matching       \n",
    "            # if sw.lower()==hw:\n",
    "            #     start_ind=i\n",
    "            #     start_inds.add(start_ind)\n",
    "\n",
    "            # if sw.lower()==tw:\n",
    "            #     end_ind=i\n",
    "            #     end_inds.add(end_ind)\n",
    "\n",
    "            # (3) plural matching\n",
    "            if 'NN' in hw_pos:\n",
    "                blob = TextBlob(hw)\n",
    "                tmp=[word.pluralize() for word in blob.words]\n",
    "                hw_plural = tmp[0] if len(tmp)>0 else ''\n",
    "                if sw==hw_plural:\n",
    "                    start_ind=i\n",
    "                    start_inds.add(start_ind)\n",
    "\n",
    "            if 'NN' in tw_pos:\n",
    "                blob = TextBlob(tw)\n",
    "                tmp=[word.pluralize() for word in blob.words]\n",
    "                tw_plural = tmp[0] if len(tmp)>0 else ''\n",
    "                if sw==tw_plural:\n",
    "                    end_ind=i\n",
    "                    end_inds.add(end_ind)\n",
    "\n",
    "            # (4) verb-tense matching\n",
    "            if 'VB' in hw_pos and hw in verb_tense:\n",
    "                tmp=verb_tense[hw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        start_ind=i\n",
    "                        start_inds.add(start_ind)\n",
    "\n",
    "            if 'VB' in tw_pos and tw in verb_tense:\n",
    "                tmp=verb_tense[tw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        end_ind=i\n",
    "                        end_inds.add(end_ind)\n",
    "\n",
    "            # (5) personal pronoun matching\n",
    "            if hw in personal_pronoun:\n",
    "                tmp=personal_pronoun[hw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        start_ind=i\n",
    "                        start_inds.add(start_ind)\n",
    "\n",
    "            if tw in personal_pronoun:\n",
    "                tmp=personal_pronoun[tw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        end_ind=i\n",
    "                        end_inds.add(end_ind)\n",
    "\n",
    "        \n",
    "        match_res.append((ss_cut, start_inds, end_inds))\n",
    "\n",
    "    # determine the final start and end indices\n",
    "    # loop over all possible combinations of start and end indices and find the one with the maximum overlap inside\n",
    "    event_inner_cut=event_text.split(' ')[1:-1]\n",
    "    start_ind=-1\n",
    "    end_ind=-1\n",
    "    best_ss_cnt=[]\n",
    "    inner_match_cnt=0\n",
    "    for ss_cut, start_inds, end_inds in match_res:\n",
    "        for s in start_inds:\n",
    "            for e in end_inds:\n",
    "                if s<e:\n",
    "                    cnt=0\n",
    "                    for _x in event_inner_cut:\n",
    "                        if _x in ss_cut[s+1:e]:\n",
    "                            cnt+=1\n",
    "                    if cnt>=inner_match_cnt:\n",
    "                        inner_match_cnt=cnt\n",
    "                        start_ind=s\n",
    "                        end_ind=e\n",
    "                        best_ss_cnt=ss_cut\n",
    "\n",
    "    event_raw_text=' '.join(best_ss_cnt[start_ind:end_ind+1])\n",
    "\n",
    "    if len(event_raw_text)>0:\n",
    "        event_processed.append((text_id, event, event_raw_text, ' '.join(best_ss_cnt), sent))\n",
    "    if len(event_raw_text)==0:\n",
    "        event_unprocessed.append((text_id, event,'','', sent))\n",
    "\n",
    "\n",
    "# tracking\n",
    "print(\"Total events: %d, processed events: %d, ratio: %.2f%%\"%(len(event_sent_list), len(event_processed), 100*len(event_processed)/len(event_sent_map)))\n",
    "\n",
    "# print(\"Unprocessed Samples: \")\n",
    "\n",
    "# __tmp=random.sample([(t,e,s) for t,e,_,s in event_unprocessed],10)\n",
    "# for t,e,s in __tmp:\n",
    "#     print(\"textID %s: %s == %s\"%(t,e.__repr__(),s))\n",
    "#     print('\\n')\n",
    "\n",
    "joblib.dump((event_processed, event_unprocessed),'Data/amazon_food_review_aser_event_restore_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9c33af12",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total events: 4136851, processed events: 3962088, ratio: 95.78%\n"
     ]
    }
   ],
   "source": [
    "print(\"Total events: %d, processed events: %d, ratio: %.2f%%\"%(len(event_sent_list), len(event_processed), 100*len(event_processed)/len(event_sent_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcaedbd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# {text_id:\n",
    "#   {event_text: [event_raw_text, sub_sent]}\n",
    "# }\n",
    "\n",
    "event_processed, event_unprocessed = joblib.load('Data/amazon_food_review_aser_event_restore_set')\n",
    "\n",
    "\n",
    "event_processed_map={}\n",
    "\n",
    "for text_id, event, event_raw_text, sub_sent, sent in event_processed:\n",
    "    if text_id not in event_processed_map:\n",
    "        event_processed_map[text_id]={}\n",
    "    event_processed_map[text_id][event.__repr__()]=[event_raw_text, sub_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "38029b85",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "554170"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(event_processed_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7517ebf0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data/amazon_food_review_aser_event_restore_map']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(event_processed_map,'Data/amazon_food_review_aser_event_restore_map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "59a31c27",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3962088"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(event_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8252bdbb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "event_processed_map[227801]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9824f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # generate 100 samples for testing \n",
    "# cnt=0\n",
    "# testset={}\n",
    "# for  text_id, doc_restores in event_processed_map.items():\n",
    "#     if cnt>100:\n",
    "#         break\n",
    "#     testset[text_id]=doc_restores\n",
    "#     cnt+=1\n",
    "# joblib.dump(testset,source_dir+'Data/amazon_food_review_aser_event_restore_testset100')\n",
    "\n",
    "# generate 100 samples for testing \n",
    "test_aser={}\n",
    "test_restore={}\n",
    "cnt=0\n",
    "for text_id, info in aser.items():\n",
    "    if cnt>100:\n",
    "        break\n",
    "    if text_id in event_processed_map:\n",
    "        test_aser[text_id]=info\n",
    "        test_restore[text_id]=event_processed_map[text_id]\n",
    "        cnt+=1\n",
    "\n",
    "joblib.dump((test_aser,test_restore),source_dir+'Data/amazon_food_review_aser_event_restore_testset100.v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90743316",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res=[]\n",
    "for text_id, info in event_processed_map.items():\n",
    "    for event, sent_list in info.items():\n",
    "        res.append(sent_list[0])\n",
    "    if len(res)>100000:\n",
    "        break\n",
    "joblib.dump(res, source_dir+'Data/test001')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415a32fa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Inquire"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "        Compute `Restore` given list of sentences and ASER events of a reviewID."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e4e6a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import tqdm\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "from mlconjug3 import Conjugator\n",
    "import re\n",
    "\n",
    "# get verb tense\n",
    "\n",
    "def get_verb_forms(conjugator, verb_word: str):\n",
    "    \"\"\"\n",
    "    compute all forms of a verb.\n",
    "\n",
    "    :param\n",
    "    :return: tuple, (present, third-person present, past, present continuous, present perfect)\n",
    "    \"\"\"\n",
    "    present, third_person_present, past, present_continuous, present_perfect = '', '', '', '', ''\n",
    "\n",
    "    try:\n",
    "        for form in conjugator.conjugate(verb_word):\n",
    "            if len(form)==4:\n",
    "                _, tense, sub, verb = form\n",
    "                if sub in ('we','they','you'):\n",
    "                    continue\n",
    "                if tense == 'indicative present' and sub == 'I':\n",
    "                    present = verb\n",
    "                if tense == 'indicative present' and sub == 'he/she/it':\n",
    "                    third_person_present = verb\n",
    "                if tense == 'indicative past tense' and sub == 'I':\n",
    "                    past = verb\n",
    "                if tense == 'indicative present continuous' and sub == 'I':\n",
    "                    present_continuous = verb\n",
    "                if tense == 'indicative present perfect' and sub == 'I':\n",
    "                    present_perfect = verb\n",
    "            else: # 3\n",
    "                continue\n",
    "        stat = 0\n",
    "    except:\n",
    "        #print(traceback.format_exc(()))\n",
    "        stat = -1\n",
    "\n",
    "    return (present, third_person_present, past, present_continuous, present_perfect), stat\n",
    "\n",
    "source_dir='./'\n",
    "\n",
    "conjugator = Conjugator(language='en')\n",
    "\n",
    "personal_pronoun={\n",
    "    'i': ['i','I','me'],\n",
    "    'you': ['you','your'],\n",
    "    'he': ['he','him','his'],\n",
    "    'she': ['she','her'],\n",
    "    'it': ['it','its'],\n",
    "    'we': ['we','us','our'],\n",
    "    'they': ['they','them','their'],\n",
    "}\n",
    "\n",
    "def clean(sentence:str):\n",
    "    res=sentence.replace(\"'ve\",' have').replace(\"'ll\",' will').replace(\"'re\",' are').replace(\"'m\",' am').replace(\"'d\",' would').replace(\"n't\",' not')\n",
    "\n",
    "    res=res.replace(\"he's\",'he has').replace(\"she's\",'she has').replace(\"it's\",'it is')\n",
    "\n",
    "    res=res.replace(\"He's\",'He has').replace(\"She's\",'She has').replace(\"It's\",'It is')\n",
    "\n",
    "    res=res.replace(\"<br />\",' ').replace(\"<br/>\",' ')\n",
    "\n",
    "    res=res.replace(\"(\",' ').replace(\")\",' ') # this apple (brand is blablabla)\n",
    "\n",
    "    res=res.replace(\";\",'.').replace(\":\",'.') # E.g.: I was excited to find these two teas because they are the perfect compromise: just enough caffeine to keep me going, but not enough to make my heart pound\n",
    "    \n",
    "    res=res.replace('\"',' ') # E.g.: I do not care for the \"Creamy Peanut Butter\"\n",
    "\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a03c61",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_id=-1\n",
    "\n",
    "sentences= # info['sentences']\n",
    "\n",
    "aser= # info['aser']\n",
    "\n",
    "event_sent_list=[]\n",
    "for s,event_list in zip(sentences,aser):\n",
    "    for e in event_list:\n",
    "        event_sent_list.append((text_id,e,s))\n",
    "        \n",
    "# get all verbs from events\n",
    "verbs=set()\n",
    "\n",
    "for text_id,event, sent in tqdm.tqdm(event_sent_list):\n",
    "\n",
    "    for w, pt in zip(event.__repr__().split(' '),event.pos_tags):\n",
    "\n",
    "        if 'VB' in pt:\n",
    "\n",
    "            verbs.add(w)\n",
    "\n",
    "verbs=list(verbs)\n",
    "print(\"Total verbs: %d\"%len(verbs))\n",
    "\n",
    "verb_tense={}\n",
    "fail=[]\n",
    "for v in tqdm.tqdm(verbs):\n",
    "    tmp,status=get_verb_forms(conjugator,v)\n",
    "    if status==-1:\n",
    "        fail.append(v)\n",
    "    else:\n",
    "        verb_tense[v]=tmp\n",
    "print(\"Failing verbs: %d\"%len(fail))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08576592",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# update: process ;:\n",
    "\n",
    "event_processed=[] # (text_id, event, raw_event_text, sub_sent, \n",
    "#sent)\n",
    "event_unprocessed=[] # (text_id, event,'', '', sent)\n",
    "\n",
    "for text_id, event, sent in tqdm.tqdm(event_sent_list):\n",
    "    event_text=event.__repr__()\n",
    "    hw=event_text.split(' ')[0]\n",
    "    hw_pos=event.pos_tags[0]\n",
    "    tw=event_text.split(' ')[-1]\n",
    "    tw_pos=event.pos_tags[-1]\n",
    "\n",
    "    sub_sents=re.split('\\.|,', clean(sent))\n",
    "\n",
    "    match_res=[]\n",
    "\n",
    "    for ss in sub_sents:\n",
    "        ss_cut=ss.split(' ')\n",
    "\n",
    "        start_inds=set()\n",
    "        end_inds=set()\n",
    "\n",
    "        for i,sw in enumerate(ss_cut):\n",
    "            sw=sw.lower()\n",
    "\n",
    "            # (1) head word and tail word matching\n",
    "            if sw==hw:\n",
    "                start_ind=i\n",
    "                start_inds.add(start_ind)\n",
    "            if sw==tw:\n",
    "                end_ind=i\n",
    "                end_inds.add(end_ind)\n",
    "            # (2) upper/lower case matching       \n",
    "            # if sw.lower()==hw:\n",
    "            #     start_ind=i\n",
    "            #     start_inds.add(start_ind)\n",
    "\n",
    "            # if sw.lower()==tw:\n",
    "            #     end_ind=i\n",
    "            #     end_inds.add(end_ind)\n",
    "\n",
    "            # (3) plural matching\n",
    "            if 'NN' in hw_pos:\n",
    "                blob = TextBlob(hw)\n",
    "                tmp=[word.pluralize() for word in blob.words]\n",
    "                hw_plural = tmp[0] if len(tmp)>0 else ''\n",
    "                if sw==hw_plural:\n",
    "                    start_ind=i\n",
    "                    start_inds.add(start_ind)\n",
    "\n",
    "            if 'NN' in tw_pos:\n",
    "                blob = TextBlob(tw)\n",
    "                tmp=[word.pluralize() for word in blob.words]\n",
    "                tw_plural = tmp[0] if len(tmp)>0 else ''\n",
    "                if sw==tw_plural:\n",
    "                    end_ind=i\n",
    "                    end_inds.add(end_ind)\n",
    "\n",
    "            # (4) verb-tense matching\n",
    "            if 'VB' in hw_pos and hw in verb_tense:\n",
    "                tmp=verb_tense[hw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        start_ind=i\n",
    "                        start_inds.add(start_ind)\n",
    "\n",
    "            if 'VB' in tw_pos and tw in verb_tense:\n",
    "                tmp=verb_tense[tw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        end_ind=i\n",
    "                        end_inds.add(end_ind)\n",
    "\n",
    "            # (5) personal pronoun matching\n",
    "            if hw in personal_pronoun:\n",
    "                tmp=personal_pronoun[hw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        start_ind=i\n",
    "                        start_inds.add(start_ind)\n",
    "\n",
    "            if tw in personal_pronoun:\n",
    "                tmp=personal_pronoun[tw]\n",
    "                for t in tmp:\n",
    "                    if sw==t:\n",
    "                        end_ind=i\n",
    "                        end_inds.add(end_ind)\n",
    "\n",
    "        \n",
    "        match_res.append((ss_cut, start_inds, end_inds))\n",
    "\n",
    "    # determine the final start and end indices\n",
    "    # loop over all possible combinations of start \n",
    "    #and end indices and find the one with the maximum overlap \n",
    "    #inside\n",
    "    event_inner_cut=event_text.split(' ')[1:-1]\n",
    "    start_ind=-1\n",
    "    end_ind=-1\n",
    "    best_ss_cnt=[]\n",
    "    inner_match_cnt=0\n",
    "    for ss_cut, start_inds, end_inds in match_res:\n",
    "        for s in start_inds:\n",
    "            for e in end_inds:\n",
    "                if s<e:\n",
    "                    cnt=0\n",
    "                    for _x in event_inner_cut:\n",
    "                        if _x in ss_cut[s+1:e]:\n",
    "                            cnt+=1\n",
    "                    if cnt>=inner_match_cnt:\n",
    "                        inner_match_cnt=cnt\n",
    "                        start_ind=s\n",
    "                        end_ind=e\n",
    "                        best_ss_cnt=ss_cut\n",
    "\n",
    "    event_raw_text=' '.join(best_ss_cnt[start_ind:end_ind+1])\n",
    "\n",
    "    if len(event_raw_text)>0:\n",
    "        event_processed.append((text_id, event, event_raw_text,\n",
    "                                ' '.join(best_ss_cnt), sent))\n",
    "    if len(event_raw_text)==0:\n",
    "        event_unprocessed.append((text_id, event,'','', sent))\n",
    "\n",
    "\n",
    "# tracking\n",
    "print(\"Total events: %d, processed events: %d, ratio: %.2f%%\"%\n",
    "      (len(event_sent_list), len(event_processed), \n",
    "       100*len(event_processed)/len(event_sent_map)))\n",
    "\n",
    "event_processed_map={}\n",
    "\n",
    "for text_id, event, event_raw_text, sub_sent, sent \\\n",
    "in event_processed:\n",
    "    if text_id not in event_processed_map:\n",
    "        event_processed_map[text_id]={}\n",
    "    event_processed_map[text_id][event.__repr__()]\\\n",
    "    =[event_raw_text, sub_sent]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "204.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}